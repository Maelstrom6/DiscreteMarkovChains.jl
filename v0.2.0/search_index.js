var documenterSearchIndex = {"docs":
[{"location":"functions/probability/#Probability-Functions","page":"Probability Functions","title":"Probability Functions","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"These functions provide probability scalars, vectors or scalars as output.","category":"page"},{"location":"functions/probability/#Contents","page":"Probability Functions","title":"Contents","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"Pages = [\"probability.md\"]","category":"page"},{"location":"functions/probability/#Index","page":"Probability Functions","title":"Index","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"Pages = [\"probability.md\"]","category":"page"},{"location":"functions/probability/#Documentation","page":"Probability Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"DiscreteMarkovChains.stationary_distribution\nDiscreteMarkovChains.exit_probabilities\nDiscreteMarkovChains.first_passage_probabilities","category":"page"},{"location":"functions/probability/#DiscreteMarkovChains.stationary_distribution","page":"Probability Functions","title":"DiscreteMarkovChains.stationary_distribution","text":"stationary_distribution(x)\n\nDefinitions\n\nA stationary distribution of a Markov chain is a probability distribution that remains unchanged in the Markov chain as time progresses. It is a row vector, w such that its elements sum to 1 and it satisfies wT = w. T is the one-step transiton matrix of the Markov chain.\n\nIn other words, w is invariant by the matrix T.\n\nFor simplicity, this function returns a column vector instead of a row vector.\n\nFor continuous Markov chains, the stationary_distribution is given by the solution to wQ = 0 where Q is the transition intensity matrix.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA column vector, w, that satisfies the equation wT = w.\n\nExamples\n\nThe stationary distribution will always exist. However, it might not be unique.\n\nIf it is unique there are no problems.\n\nusing DiscreteMarkovChains\nT = [\n    4 2 4;\n    1 0 9;\n    3 5 2;\n]//10\nX = DiscreteMarkovChain(T)\n\nstationary_distribution(X)\n\n# output\n\n3-element Array{Rational{Int64},1}:\n 35//129\n 12//43\n 58//129\n\nIf there are infinite solutions then the principle solution is taken (every free variable is set to 0). A Moore-Penrose inverse is used.\n\nT = [\n    0.4 0.6 0.0;\n    0.6 0.4 0.0;\n    0.0 0.0 1.0;\n]\nX = DiscreteMarkovChain(T)\n\nstationary_distribution(X)\n\n# output\n\n3-element Array{Float64,1}:\n 0.33333333333333337\n 0.33333333333333337\n 0.33333333333333337\n\nReferences\n\nBrilliant.org\n\n\n\n\n\n","category":"function"},{"location":"functions/probability/#DiscreteMarkovChains.exit_probabilities","page":"Probability Functions","title":"DiscreteMarkovChains.exit_probabilities","text":"exit_probabilities(x)\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nAn array where element (i j) is the probability that transient state i will enter recurrent state j on its first step out of the transient states. That is, e_ij.\n\nExamples\n\nThe following should be fairly obvious. States 1, 2 and 3 are the recurrent states and state 4 is the single transient state that must enter one of these 3 on the next time step. There is no randomness at play here.\n\nusing DiscreteMarkovChains\nT = [\n    0.2 0.2 0.6 0.0;\n    0.5 0.4 0.1 0.0;\n    0.6 0.2 0.2 0.0;\n    0.2 0.3 0.5 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nexit_probabilities(X)\n\n# output\n\n1×3 Array{Float64,2}:\n 0.2  0.3  0.5\n\nSo state 4 has probabilities 0.2, 0.3 and 0.5 of reaching states 1, 2 and 3 respectively on the first step out of the transient states (consisting only of state 4).\n\nThe following is less obvious.\n\nT = [\n    1.0 0.0 0.0 0.0;\n    0.0 1.0 0.0 0.0;\n    0.1 0.3 0.3 0.3;\n    0.2 0.3 0.4 0.1;\n]\nX = DiscreteMarkovChain(T)\n\nexit_probabilities(X)\n\n# output\n\n2×2 Array{Float64,2}:\n 0.294118  0.705882\n 0.352941  0.647059\n\nSo state 3 has a 29% chance of entering state 1 on the first time step out (and the remaining 71% chance of entering state 2). State 4 has a 35% chance of reaching state 1 on the first time step out.\n\n\n\n\n\n","category":"function"},{"location":"functions/probability/#DiscreteMarkovChains.first_passage_probabilities","page":"Probability Functions","title":"DiscreteMarkovChains.first_passage_probabilities","text":"first_passage_probabilities(x, t, i=missing, j=missing)\n\nDefinitions\n\nThis is the probability that the process enters state j for the first time at time t given that the process started in state i at time 0. That is, f^(t)_ij. If no i or j is given, then it will return a matrix instead with entries f^(t)_ij for i and j in the state space of x.\n\nWhy Do We Use A Slow Algorithm?\n\nSo that t can be symbolic if nessesary. That is, if symbolic math libraries want to use this library, it will pose no hassle.\n\nArguments\n\nx: some kind of Markov chain.\nt: the time to calculate the first passage probability.\ni: the state that the prcess starts in.\nj: the state that the process must reach for the first time.\n\nReturns\n\nA scalar value or a matrix depending on whether i and j are given.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    0.1 0.9;\n    0.3 0.7;\n]\nX = DiscreteMarkovChain(T)\n\nfirst_passage_probabilities(X, 2)\n\n# output\n\n2×2 Array{Float64,2}:\n 0.27  0.09\n 0.21  0.27\n\nIf X has a custom state space, then i and j must be in that state space.\n\nT = [\n    0.1 0.9;\n    0.3 0.7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Rainy\"], T)\n\nfirst_passage_probabilities(X, 2, \"Sunny\", \"Rainy\")\n\n# output\n\n0.09000000000000001\n\nNotice how this is the (1, 2) entry in the first example.\n\nReferences\n\nUniversity of Windsor\nDurham University\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#Core-Types","page":"Core Types","title":"Core Types","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"These are types that are most often used. They are simple and are used by all other functions.","category":"page"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"There is basic support for continuous Markov chains. All applicable functions also work for continuous chains.","category":"page"},{"location":"functions/basic/#Contents","page":"Core Types","title":"Contents","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"Pages = [\"basic.md\"]","category":"page"},{"location":"functions/basic/#Index","page":"Core Types","title":"Index","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"Pages = [\"basic.md\"]","category":"page"},{"location":"functions/basic/#Documentation","page":"Core Types","title":"Documentation","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"The main type is DiscreteMarkovChain:","category":"page"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"DiscreteMarkovChains.DiscreteMarkovChain\nDiscreteMarkovChains.ContinuousMarkovChain\n\nDiscreteMarkovChains.state_space\nDiscreteMarkovChains.transition_matrix\nDiscreteMarkovChains.probability_matrix\n\nDiscreteMarkovChains.embedded","category":"page"},{"location":"functions/basic/#DiscreteMarkovChains.DiscreteMarkovChain","page":"Core Types","title":"DiscreteMarkovChains.DiscreteMarkovChain","text":"DiscreteMarkovChain(transition_matrix)\nDiscreteMarkovChain(state_space, transition_matrix)\nDiscreteMarkovChain(continuous_markov_chain)\n\nCreates a new discrete Markov chain object.\n\nArguments\n\nstate_space: The names of the states that make up the Markov chain.\ntransition_matrix: The single step transition probability matrix.\ncontinuous_markov_chain: An instance of ContinuousMarkovChain.\n\nExamples\n\nThe following shows a basic Sunny-Cloudy-Rainy weather model.\n\nusing DiscreteMarkovChains\nT = [\n    0.9 0.1 0;\n    0.5 0.2 0.3;\n    0.1 0.4 0.5\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\nprintln(state_space(X))\n\n# output\n\n[\"Sunny\", \"Cloudy\", \"Rainy\"]\n\nprintln(transition_matrix(X))\n\n# output\n\n[0.9 0.1 0.0; 0.5 0.2 0.3; 0.1 0.4 0.5]\n\nReferences\n\nWikipedia\nDartmouth College\n\n\n\n\n\n","category":"type"},{"location":"functions/basic/#DiscreteMarkovChains.ContinuousMarkovChain","page":"Core Types","title":"DiscreteMarkovChains.ContinuousMarkovChain","text":"ContinuousMarkovChain(transition_matrix)\nContinuousMarkovChain(state_space, transition_matrix)\nContinuousMarkovChain(discrete_markov_chain)\n\nCreates a new continuous Markov chain object. This is also known as a Markov jump process.\n\nNote that an irreducible finite continuous-time Markov chain is always positive recurrent and its stationary distribution always exists, is unique and is equal to the limiting distribution.\n\nArguments\n\nstate_space: The names of the states that make up the Markov chain.\ntransition_matrix: The transition intensity matrix. Also known as the generator matrix.\ndiscrete_markov_chain: An instance of DiscreteMarkovChain.\n\nExamples\n\nThe following shows a basic Sunny-Cloudy-Rainy weather model.\n\nusing DiscreteMarkovChains\nT = [\n    -.1 0.1 0.0;\n    0.5 -.8 0.3;\n    0.1 0.4 -.5;\n]\nX = ContinuousMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\nprintln(state_space(X))\n\n# output\n\n[\"Sunny\", \"Cloudy\", \"Rainy\"]\n\nprintln(transition_matrix(X))\n\n# output\n\n[-0.1 0.1 0.0; 0.5 -0.8 0.3; 0.1 0.4 -0.5]\n\nReferences\n\nWikipedia\n\n\n\n\n\n","category":"type"},{"location":"functions/basic/#DiscreteMarkovChains.state_space","page":"Core Types","title":"DiscreteMarkovChains.state_space","text":"state_space(x)\n\nDefinitions\n\nThe state space of a Markov chain is the (ordered) set of values that the process is able to take on. For example, in a Sunny-Cloudy-Rainy weather model, the state space is [\"Sunny\", \"Cloudy\", \"Rainy\"].\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe state space of the Markov chain.\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#DiscreteMarkovChains.transition_matrix","page":"Core Types","title":"DiscreteMarkovChains.transition_matrix","text":"transition_matrix(x)\n\nDefinitions\n\nThe one-step transition matrix, T, of a discrete Markov chain or the generator matrix of a continuous Markov chain.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe transition matrix of the Markov chain.\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#DiscreteMarkovChains.probability_matrix","page":"Core Types","title":"DiscreteMarkovChains.probability_matrix","text":"probability_matrix(x)\n\nDefinitions\n\nThe one-step transition matrix, T, of a Markov chain, X_t is a matrix whose (ij)th entry is the probability of the process being in state j at time 1 given that the process started in state i at time 0. That is\n\nT = p^(1)_ij_ij  S = mathbbP(X_1=j  X_0=i)_ij  S\n\nwhere S is the state space of X_t.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe one-step probability matrix of the Markov chain.\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#DiscreteMarkovChains.embedded","page":"Core Types","title":"DiscreteMarkovChains.embedded","text":"embedded(x)\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nIf the Markv chain is continuous, then it returns the embedded Markov chain. If the Markov chain is discrete, then it returns the given chain, x.\n\nNotes\n\nIf the equivalent chain is preffered rather than the embedded chain, then use DiscreteMarkovChain(x).\n\n\n\n\n\n","category":"function"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributing is welcomed with open arms. This helps the package become better and simply by working on it, you are increasing its popularity.","category":"page"},{"location":"contributing/#Coding-Style","page":"Contributing","title":"Coding Style","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"One should see the Julia style guide as an introduction. The overall style used it the Blue style guide.","category":"page"},{"location":"contributing/#Exceptions-To-Blue","page":"Contributing","title":"Exceptions To Blue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Instead of leading underscores for names of private functions, we keep them as is. For example is_row_stochastic should be called _is_row_stochastic under the Blue style guide. This function is simply not exported. Since Julia allows for massive code reuse, there is a chance that a developer might want to make use of our private functions (which are actually more useful than the public ones for this package).\nWe will implicitly return nothing in functions. The Blue style guide requires this to be explicit. This is to make code look cleaner.\nInline comments that do not form a full sentence can start with a small letter or a capital letter. They must not end with a full stop. This is to match with the Blue style guide and with Julia's source code itself.\nURLs do not need to adhere to the 92-character line-limit.","category":"page"},{"location":"contributing/#Additions-To-Blue","page":"Contributing","title":"Additions To Blue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All struct types should inherit from an abstract type. This makes inheritance easier for other developers using our code.\nOnly have abstract types in type annotations in functions and struct types. This makes it easier for other developers using our code.\nTry to make every element have the same character length in a matrix. See the following\n# yes\nT = [\n    0.0 1.0;\n    0.5 2.3;\n]\n\n# yes\nT = [\n    .12 .35;\n    .55 .90;\n]\n\n# no\nT = [\n    0 1.0;\n    0.5 2.3;\n]\n\n# no\nT = [\n    .12 .35;\n    .55 .9;\n]\nThis makes reading matrices easier. If you are ever unsure, do whatever looks the best.\nThe following headings are to be used for docstrings:\nThe methods of the function indented by 4 spaces.\nA brief explanation on the function.\nDefinitions\nParameters\nKeywords\nReturns\nThrows\nNotes\nExamples\nReferences\nNot all of these need to be present, but where applicable, they should follow this order. Headings must have a single empty line directly above them. They should come directly after a single hash (#) and space on the same line. They must have no empty lines directly below them.","category":"page"},{"location":"advanced/#Advanced","page":"Advanced","title":"Advanced","text":"","category":"section"},{"location":"advanced/#Custom-Element-Types","page":"Advanced","title":"Custom Element Types","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"This library also supports transition matrices whose elements are user-defined objects or even matrices. See test/Advanced.jl for examples.","category":"page"},{"location":"advanced/#Conversion-Between-Discrete-And-Continuous","page":"Advanced","title":"Conversion Between Discrete And Continuous","text":"","category":"section"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"There is conversion ability between types. This can be done via convert or simply calling the class to convert to:","category":"page"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"using DiscreteMarkovChains\n\nT = [\n    0.3 0.7 0.0;\n    0.5 0.0 0.5;\n    0.0 1.0 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nprobability_matrix(X)\n\n# output\n\n3×3 Array{Float64,2}:\n 0.3  0.7  0.0\n 0.5  0.0  0.5\n 0.0  1.0  0.0","category":"page"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"Y = convert(ContinuousMarkovChain, X)\nY = ContinuousMarkovChain(X)  # Same operation\n\nround.(abs.(probability_matrix(Y)), digits=2)\n\n# output\n\n3×3 Array{Float64,2}:\n 0.3  0.7  0.0\n 0.5  0.0  0.5\n 0.0  1.0  0.0","category":"page"},{"location":"advanced/","page":"Advanced","title":"Advanced","text":"The conversion is done so that the one-step transition probability matrix remains the same. If you want the embedded chain, then simply use embedded(Y).","category":"page"},{"location":"functions/simple/#Boolean-Functions","page":"Boolean Functions","title":"Boolean Functions","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"These functions have a boolean output. They are fairly simple and tend to categorize Markov chains into various labels.","category":"page"},{"location":"functions/simple/#Contents","page":"Boolean Functions","title":"Contents","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"Pages = [\"simple.md\"]","category":"page"},{"location":"functions/simple/#Index","page":"Boolean Functions","title":"Index","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"Pages = [\"simple.md\"]","category":"page"},{"location":"functions/simple/#Documentation","page":"Boolean Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"DiscreteMarkovChains.is_regular\nDiscreteMarkovChains.is_ergodic\nDiscreteMarkovChains.is_absorbing\nDiscreteMarkovChains.is_reversible","category":"page"},{"location":"functions/simple/#DiscreteMarkovChains.is_regular","page":"Boolean Functions","title":"DiscreteMarkovChains.is_regular","text":"is_regular(x)\n\nDefinitions\n\nA Markov chain is called a regular chain if some power of the transition matrix has only positive elements. This is equivalent to being ergodic and aperiodic.\n\nArguments\n\nx: some kind of discrete Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is regular.\n\nExamples\n\nWe will set up a matrix with 2 communication classes and show that it is not regular.\n\nusing DiscreteMarkovChains\nT = [\n    0 1 0;\n    1 0 0;\n    0 0 1;\n]\nX = DiscreteMarkovChain(T)\n\nis_regular(X)\n\n# output\n\nfalse\n\nRepeat the above but now all states communicate.\n\nT = [\n    0.0 0.5 0.5;\n    0.0 0.0 1.0;\n    1.0 0.0 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nis_regular(X)\n\n# output\n\ntrue\n\nNotice how a periodic chain is not regular even though there is only one communication class.\n\nT = [\n    0 1 0;\n    0 0 1;\n    1 0 0;\n]\nX = DiscreteMarkovChain(T)\n\nis_regular(X)\n\n# output\n\nfalse\n\n\n\n\n\n","category":"function"},{"location":"functions/simple/#DiscreteMarkovChains.is_ergodic","page":"Boolean Functions","title":"DiscreteMarkovChains.is_ergodic","text":"is_ergodic(x)\n\nDefinitions\n\nA Markov chain is called an ergodic chain if it is possible to go from every state to every state (not necessarily in one move). That is, every state communicates and the whole tranistion matrix forms one communication class.\n\nIn many books, ergodic Markov chains are called irreducible.\n\nIf a Markov chain is not irreducible then it is reducible. This means that the Markov chain can be broken down into smaller irreducible chains that do not communicate. One chain might still be accessible from another though.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is ergodic. This is, if every state can be accessed from every other state. Another term for this is irreducible.\n\nExamples\n\nWe will set up a matrix with 2 communication classes and show that it is not ergodic.\n\nusing DiscreteMarkovChains\nT = [\n    0 1 0;\n    1 0 0;\n    0 0 1;\n]\nX = DiscreteMarkovChain(T)\n\nis_ergodic(X)\n\n# output\n\nfalse\n\nRepeat the above but now all states communicate.\n\nT = [\n    0.0 0.5 0.5;\n    0.0 0.0 1.0;\n    1.0 0.0 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nis_ergodic(X)\n\n# output\n\ntrue\n\nNotice how a periodic chain is regular no matter the periodicity.\n\nT = [\n    0 1 0;\n    0 0 1;\n    1 0 0;\n]\nX = DiscreteMarkovChain(T)\n\nis_ergodic(X)\n\n# output\n\ntrue\n\n\n\n\n\n","category":"function"},{"location":"functions/simple/#DiscreteMarkovChains.is_absorbing","page":"Boolean Functions","title":"DiscreteMarkovChains.is_absorbing","text":"is_absorbing(x)\n\nDefinitions\n\nA Markov chain is absorbing if it has at least one absorbing state, and if from every state it is possible to go to an absorbing state (not necessarily in one step).\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is an absorbing chain. So the process is guarenteed to be absorbed eventually.\n\nExamples\n\nThe following is a typical example of an absorbing chain.\n\nusing DiscreteMarkovChains\nT = [\n    1.0 0.0 0.0;\n    0.1 0.8 0.1;\n    0.0 0.3 0.7;\n]\nX = DiscreteMarkovChain(T)\n\nis_absorbing(X)\n\n# output\n\ntrue\n\nIf a Markov chain does not have an absorbing state then the chain is not absorbing. The converse is not true as we will see soon.\n\nT = [\n    0.5 0.5 0.0;\n    0.5 0.0 0.5;\n    0.0 0.5 0.5;\n]\nX = DiscreteMarkovChain(T)\n\nis_absorbing(X)\n\n# output\n\nfalse\n\nIn the following, the chain has multiple absorbing states but the process is not guarenteed to be absorbed into those states.\n\nT = [\n    1 0 0 0;\n    0 1 0 0;\n    0 0 0 1;\n    0 0 1 0;\n]\nX = DiscreteMarkovChain(T)\n\nis_absorbing(X)\n\n# output\n\nfalse\n\nReferences\n\nDartmouth College\n\n\n\n\n\n","category":"function"},{"location":"functions/simple/#DiscreteMarkovChains.is_reversible","page":"Boolean Functions","title":"DiscreteMarkovChains.is_reversible","text":"is_reversible(x)\n\nDefinitions\n\nA Markov chain is said to be reversible if it satisfies the equation π_i p_ij = π_j p_ji where π is the stationary distribution and p are the elements of the transition matrix of the Markov chain.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain is reversible.\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#Mean-Time-Functions","page":"Mean Time Functions","title":"Mean Time Functions","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"These functions provide scalars, vectors or matrices representing expected times for some event.","category":"page"},{"location":"functions/mean/#Contents","page":"Mean Time Functions","title":"Contents","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"Pages = [\"mean.md\"]","category":"page"},{"location":"functions/mean/#Index","page":"Mean Time Functions","title":"Index","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"Pages = [\"mean.md\"]","category":"page"},{"location":"functions/mean/#Documentation","page":"Mean Time Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"DiscreteMarkovChains.fundamental_matrix\nDiscreteMarkovChains.mean_time_to_absorption\nDiscreteMarkovChains.mean_recurrence_time\nDiscreteMarkovChains.mean_first_passage_time","category":"page"},{"location":"functions/mean/#DiscreteMarkovChains.fundamental_matrix","page":"Mean Time Functions","title":"DiscreteMarkovChains.fundamental_matrix","text":"fundamental_matrix(x)\n\nDefinitions\n\nThe fundamental matrix of a markov chain is defined to be (I-C)^-1 where C is the sub-transition matrix that takes transient states to transient states.\n\nThe (i j)th entry of the fundamental matrix is the expected number of times the chain is in state j over the whole process given that the chain started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe fundamental matrix of the Markov chain.\n\nNotes\n\nIt is advised to have x in canonical form already. This is to avoid confusion of what states make up each element of the array ouput.\n\nExamples\n\nHere is a typical example of the fundamental matrix.\n\nusing DiscreteMarkovChains\nT = [\n    1.0 0.0 0.0;\n    0.0 0.4 0.6;\n    0.2 0.2 0.6;\n]\nX = DiscreteMarkovChain(T)\n\nfundamental_matrix(X)\n\n# output\n\n2×2 Array{Float64,2}:\n 3.33333  5.0\n 1.66667  5.0\n\nIf the chain is ergodic, then the fundamental matrix is a 0x0 matrix (since there are no transient states).\n\nT = [\n    0.0 1.0 0.0;\n    0.5 0.0 0.5;\n    0.0 1.0 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nfundamental_matrix(X)\n\n# output\n\n0×0 Array{Any,2}\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#DiscreteMarkovChains.mean_time_to_absorption","page":"Mean Time Functions","title":"DiscreteMarkovChains.mean_time_to_absorption","text":"mean_time_to_absorption(x)\n\nDefinitions\n\nThe expected time to absorption is the expected number of steps that the process will take while in the transient super class given that the process started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array where element i is the total number of revisits to transient state i before leaving the transient super class.\n\nNotes\n\nIt is advised to have x in canonical form already. This is to avoid confusion of what states make up each element of the array ouput.\n\nExamples\n\nSee the following where we have a chain with 2 transient states (states 3 and 4) that go between eachother. One of those states will enter a pre-absorbing state (state 2). And then the pre-absorbing state will enter the absorbing state (state 1) on the next step.\n\nSo state 2 should spend no more steps in the transient states and hence should have a time to absorption of 0. State 4 should have 1 more step than state 3 since state 4 must enter state 3 to exit the transient states.\n\nusing DiscreteMarkovChains\nT = [\n    1.0 0.0 0.0 0.0;\n    1.0 0.0 0.0 0.0;\n    0.0 0.2 0.0 0.8;\n    0.0 0.0 1.0 0.0;\n]\nX = DiscreteMarkovChain(T)\n\nmean_time_to_absorption(X)\n\n# output\n\n3-element Array{Float64,1}:\n  0.0\n  9.000000000000002\n 10.000000000000002\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#DiscreteMarkovChains.mean_recurrence_time","page":"Mean Time Functions","title":"DiscreteMarkovChains.mean_recurrence_time","text":"mean_recurrence_time(x)\n\nDefinitions\n\nThis is the expected number of steps for the process to return to state i given that the process started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array where the ith entry is the mean recurrence time of state i.\n\nNotes\n\nx must be irreducible (i.e. ergodic).\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    0.1 0.2 0.7;\n    0.3 0.0 0.7;\n    0.4 0.4 0.2;\n]\nX = DiscreteMarkovChain(T)\n\nmean_recurrence_time(X)\n\n# output\n\n3-element Array{Float64,1}:\n 3.4615384615384603\n 4.090909090909091\n 2.1428571428571432\n\nIf we have a reducible chain, then no error will be raised but the output will be nonsense.\n\nT = [\n    1.0 0.0 0.0;\n    0.1 0.5 0.4;\n    0.0 0.3 0.7;\n]\nX = DiscreteMarkovChain(T)\n\nmean_recurrence_time(X)\n\n# output\n\n3-element Array{Float64,1}:\n   1.0\n -Inf\n -Inf\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#DiscreteMarkovChains.mean_first_passage_time","page":"Mean Time Functions","title":"DiscreteMarkovChains.mean_first_passage_time","text":"mean_first_passage_time(x)\n\nDefinitions\n\nThis is the expected number of steps for the process to reach state j given that the process started in state i. We say that the mean first passage time between a state and itself is 0.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA matrix where the (ij)th entry is the mean recurrence time of state i. Diagonal elements are 0. The diagonals would have represented the mean recurrence time.\n\nNotes\n\nx must be irreducible (i.e. ergodic).\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    0.1 0.2 0.7;\n    0.3 0.0 0.7;\n    0.4 0.4 0.2;\n]\nX = DiscreteMarkovChain(T)\n\nmean_first_passage_time(X)\n\n# output\n\n3×3 Array{Float64,2}:\n 0.0      3.40909  1.42857\n 2.88462  0.0      1.42857\n 2.69231  2.95455  0.0\n\nIf we have a reducible chain, then no error will be raised but the output will be nonsense.\n\nT = [\n    1.0 0.0 0.0;\n    0.1 0.5 0.4;\n    0.0 0.3 0.7;\n]\nX = DiscreteMarkovChain(T)\n\nmean_first_passage_time(X)\n\n# output\n\n3×3 Array{Float64,2}:\n  0.0     -Inf  -Inf\n 23.3333  NaN   -Inf\n 26.6667  -Inf  NaN\n\n\n\n\n\n","category":"function"},{"location":"#DiscreteMarkovChains","page":"Home","title":"DiscreteMarkovChains","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = DiscreteMarkovChains","category":"page"},{"location":"","page":"Home","title":"Home","text":"DiscreteMarkovChains is a package that supports various functions relating to discrete Markov chains. In particular, it deals with discrete-time discrete-space time-homogenous finite Markov chains.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This library also deals with continuous Markov chains. Any function in the documentation that takes \"some kind of Markov chain\" as an argument can be a DiscreteMarkovChain or a ContinuousMarkovChain. Sadly there are very few examples for continuous Markov chains but they operate in the same way as discrete Markov chains.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiscreteMarkovChains should be up on Julia's package registry.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simply type ] add DiscreteMarkovChains into the Julia REPL.","category":"page"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the documentation hosted on GitHub Pages.","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#Discrete-Time","page":"Home","title":"Discrete Time","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We want to find out if this chain is an absorbing chain.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DiscreteMarkovChains\n\ntransition_matrix = [\n    0.0 1.0 0.0;\n    0.5 0.0 0.5;\n    0.0 1.0 0.0;\n]\nchain = DiscreteMarkovChain(transition_matrix)\nis_absorbing(chain)\n\n# output\n\nfalse","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's try find the communication classes, see if they are recurrent and what their periodicity is.","category":"page"},{"location":"","page":"Home","title":"Home","text":"periodicities(chain)\n\n# output\n\n([[1, 2, 3]], Any[true], Any[2])","category":"page"},{"location":"","page":"Home","title":"Home","text":"This means that we have one communication class with 3 recurrent states. Their periodicity is 2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since we have a single communication class, we can calculate the mean recurrence times.","category":"page"},{"location":"","page":"Home","title":"Home","text":"mean_recurrence_time(chain)\n\n# output\n\n3-element Array{Float64,1}:\n 4.0\n 2.0\n 4.0","category":"page"},{"location":"","page":"Home","title":"Home","text":"So the first and third states take an average of 4 time steps to return to itself. The second state takes an average of 2 steps to return to itself.","category":"page"},{"location":"#Continuous-Time","page":"Home","title":"Continuous Time","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There is support for continuous Markov chains as well.","category":"page"},{"location":"","page":"Home","title":"Home","text":"generator = [\n    -3 1 2;\n    0 -1 1;\n    1 1 -2;\n]\nchain = ContinuousMarkovChain(generator)\n\ncommunication_classes(chain)\n\n# output\n\n([[1, 2, 3]], Any[true])","category":"page"},{"location":"","page":"Home","title":"Home","text":"So we have one communication class that is recurrent.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Calculate the stationary distribution of the chain.","category":"page"},{"location":"","page":"Home","title":"Home","text":"stationary_distribution(chain)\n\n# output\n\n3-element Array{Float64,1}:\n 0.125\n 0.5\n 0.375","category":"page"},{"location":"","page":"Home","title":"Home","text":"Calculate the mean first passage time of the chain.","category":"page"},{"location":"","page":"Home","title":"Home","text":"round.(mean_first_passage_time(chain), digits=2)\n\n# output\n\n3×3 Array{Float64,2}:\n 0.0  1.0  0.67\n 3.0  0.0  1.0\n 2.0  1.0  0.0","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Chris du Plessis","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MIT","category":"page"},{"location":"functions/internal/#Internal-Functions","page":"Internal Functions","title":"Internal Functions","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"These functions are not intended to be used by you, the user.","category":"page"},{"location":"functions/internal/#Contents","page":"Internal Functions","title":"Contents","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"functions/internal/#Index","page":"Internal Functions","title":"Index","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"functions/internal/#Documentation","page":"Internal Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"The main type is DiscreteMarkovChain:","category":"page"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"DiscreteMarkovChains.strongly_connected_components\nDiscreteMarkovChains.breadth_first_search\n\nDiscreteMarkovChains.digraph\n\nDiscreteMarkovChains.state_index\nDiscreteMarkovChains.is_row_stochastic\n\nDiscreteMarkovChains.required_row_sum\nDiscreteMarkovChains.characteristic_matrix","category":"page"},{"location":"functions/internal/#DiscreteMarkovChains.strongly_connected_components","page":"Internal Functions","title":"DiscreteMarkovChains.strongly_connected_components","text":"strongly_connected_components(V::Array, E::Array{Tuple})\n\nStrongly connected components of a directed graph in reverse topological order. Translated from sympy/utilities/iteratbles.py/stronglyconnectedcomponents.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.breadth_first_search","page":"Internal Functions","title":"DiscreteMarkovChains.breadth_first_search","text":"Computes the period of an irreducible transition matrix T must be a 2D array\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.digraph","page":"Internal Functions","title":"DiscreteMarkovChains.digraph","text":"digraph(x)\n\nCreates a digraph (directed graph) representation of a Markov chain.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array of 2-tuples. An element (i j) is in the array iff the transition matrix at (ij) is nonzero.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.state_index","page":"Internal Functions","title":"DiscreteMarkovChains.state_index","text":"state_index(x)\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA dictionary mapping each state in a Markov chain to its position in the state space. It is essentially the inverse of state_space(x).\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.is_row_stochastic","page":"Internal Functions","title":"DiscreteMarkovChains.is_row_stochastic","text":"is_row_stochastic(mat, row_sum=1)\n\nTests whether a matrix, x, is row stochasitc. The desired sum of each row can be specified as well.\n\nDefinitions\n\nA matrix is said to be row stochasic if all its rows sum to 1. This definition is extened so that all its rows sum to row_sum.\n\nArguments\n\nmat: a matrix that we want to check.\nrow_sum: the desired value that each row should total to.\n\nReturns\n\ntrue if the given matrix, mat, is row-stochasitc.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.required_row_sum","page":"Internal Functions","title":"DiscreteMarkovChains.required_row_sum","text":"required_row_sum(type)\n\nArguments\n\ntype: The type of Markov chain. It can be\n\nDiscreteMarkovChain or ContinuousMarkovChain.\n\nReturns\n\nThe number that each row in the transition matrix should sum up to.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.characteristic_matrix","page":"Internal Functions","title":"DiscreteMarkovChains.characteristic_matrix","text":"characteristic_matrix(::AbstractDiscreteMarkovChain)\ncharacteristic_matrix(::AbstractContinuousMarkovChain)\n\nDefinitions\n\nMany derivations and interesting ideas about Markov chains involve the identity matrix or zero matrix somewhere along the line. Most of the time, the identity matrix appears more often in discrete Markov chains. Instead of the identity matrix, the zero matrix appears in its place for continuous Markov chains.\n\nReturns\n\nThe identity matrix if its argument is an instance of AbstractDiscreteMarkovChain. The zero matrix if its argument is an instance of AbstractContinuousMarkovChain\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#Communication-Functions","page":"Communication Functions","title":"Communication Functions","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"These functions deal with the structure of the transition matrix. It identifies how classes communicate, whether they are recurrent or transient and what their periodicity is.","category":"page"},{"location":"functions/communication/#Contents","page":"Communication Functions","title":"Contents","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"Pages = [\"communication.md\"]","category":"page"},{"location":"functions/communication/#Index","page":"Communication Functions","title":"Index","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"Pages = [\"communication.md\"]","category":"page"},{"location":"functions/communication/#Documentation","page":"Communication Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"DiscreteMarkovChains.communication_classes\nDiscreteMarkovChains.periodicities\nDiscreteMarkovChains.decompose\nDiscreteMarkovChains.canonical_form","category":"page"},{"location":"functions/communication/#DiscreteMarkovChains.communication_classes","page":"Communication Functions","title":"DiscreteMarkovChains.communication_classes","text":"communication_classes(x)\n\nDefinitions\n\nA state j is accessible from state i if it is possible to eventually reach state j given that the process started in state i. That is,   t  mathbbN such that p^(t)_ij  0.\n\nStates i and j communicate if i is accessible from j and j is accessible from i.\n\nA communication class is the set of states in a Markov chain that are all accessible from one another. Communication classes form a class in the mathematical sense. They also form a partition of the state space.\n\nA state i is recurrent if the process is guarenteed to eventually return to state i given that the process started in state i. If a state i is not recurrent, then it is said to be transient.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple containing 2 arrays.\n\nThis first array contains C arrays which store the states that communicate.\nThe second array is an array of Bool where the ith value is true if the ith communication class is recurrent.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    1 0;\n    0 1;\n]\nX = DiscreteMarkovChain(T)\n\ncommunication_classes(X)\n\n# output\n\n([[1], [2]], Any[true, true])\n\nSo the Markov chain has two communication classes and both are recurrent.\n\nT = [\n    .5 .5 .0;\n    .2 .8 .0;\n    .1 .2 .7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\ncommunication_classes(X)\n\n# output\n\n([[\"Sunny\", \"Cloudy\"], [\"Rainy\"]], Any[true, false])\n\nSo the Sunny and Cloudy states communicate and are recurrent. The Rainy state is transient. Note that this is not a very good weather model since once it stops raining, it will never rain again.\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.periodicities","page":"Communication Functions","title":"DiscreteMarkovChains.periodicities","text":"periodicities(x::AbstractDiscreteMarkovChain)\n\nA more advanced version of communication_classes designed for discrete Markov chains. It is the same as communication_classes but it returns periodicities as well.\n\nDefinitions\n\nThe period, d_i of a state i is the greatest common denominator of all integers n  mathbbN for which p^(n)_ii  0. Written more succinctly,\n\nd_i = textgcd n  mathbbN  p^(n)_ii  0 \n\nIf d_i=1 then state i is said to be aperiodic.\n\nArguments\n\nx: some kind of discrete Markov chain.\n\nReturns\n\nA tuple containing 3 arrays.\n\nThis first array contains C arrays which store the states that communicate.\nThe second array is an array of Bool where the ith value is true if the ith communication class is recurrent.\nThe third array is the periodicity of each communication class.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    1 0;\n    0 1;\n]\nX = DiscreteMarkovChain(T)\n\nperiodicities(X)\n\n# output\n\n([[1], [2]], Any[true, true], Any[1, 1])\n\nSo the Markov chain has two communication classes and both are recurrent and aperiodic.\n\nT = [\n    0.0 1.0 0.0;\n    1.0 0.0 0.0;\n    0.1 0.2 0.7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\nperiodicities(X)\n\n# output\n\n([[\"Sunny\", \"Cloudy\"], [\"Rainy\"]], Any[true, false], Any[2, 1])\n\nSo the Sunny and Cloudy states communicate and are recurrent with period 2. The Rainy state is transient and aperiodic. Note that this is not a very good weather model since once it stops raining, it will never rain again. Also, each day after that, the process will oscillate between Sunny and Cloudy.\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.decompose","page":"Communication Functions","title":"DiscreteMarkovChains.decompose","text":"decompose(x)\n\nAll transition matrices can be reordered so that we have\n\nT = beginpmatrix\nA  0\nB  C\nendpmatrix\n\nWhere A, B and C are as described below. 0 is a matrix of zeros.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple of four values\n\nstates: the first value is an array of the new states.\nA: the second value is a matrix of recurrent states to recurrent states.\nB: the third value is a matrix of transient states to recurrent states.\nC: the fourth value is a matrix of transient to transient states.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    0.0 1.0 0.0;\n    1.0 0.0 0.0;\n    0.1 0.2 0.7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\ndecompose(X)\n\n# output\n\n(Any[\"Sunny\", \"Cloudy\", \"Rainy\"], [0.0 1.0; 1.0 0.0], [0.1 0.2], [0.7])\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.canonical_form","page":"Communication Functions","title":"DiscreteMarkovChains.canonical_form","text":"canonical_form(x)\n\nReorders the states of the transition matrix of x so that recurrent states appear first and transient states appear last.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple with the new states and the new transition matrix.\n\nExamples\n\nWe will use the same matrix as previous examples but change the order of it.\n\nusing DiscreteMarkovChains\nT = [\n    0.7 0.2 0.1;\n    0.0 0.0 1.0;\n    0.0 1.0 0.0;\n]\nX = DiscreteMarkovChain([\"Rainy\", \"Cloudy\", \"Sunny\"], T)\n\ncanonical_form(X)\n\n# output\n\n(Any[\"Cloudy\", \"Sunny\", \"Rainy\"], [0.0 1.0 0.0; 1.0 0.0 0.0; 0.2 0.1 0.7])\n\nHere, Cloudy and Sunny are recurrent states so they appear first. Rainy is a transient state so it appears last.\n\n\n\n\n\n","category":"function"}]
}
