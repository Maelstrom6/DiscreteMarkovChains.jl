var documenterSearchIndex = {"docs":
[{"location":"functions/probability/#Probability-Functions","page":"Probability Functions","title":"Probability Functions","text":"","category":"section"},{"location":"functions/probability/#Contents","page":"Probability Functions","title":"Contents","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"Pages = [\"probability.md\"]","category":"page"},{"location":"functions/probability/#Index","page":"Probability Functions","title":"Index","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"Pages = [\"probability.md\"]","category":"page"},{"location":"functions/probability/#Documentation","page":"Probability Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/probability/","page":"Probability Functions","title":"Probability Functions","text":"DiscreteMarkovChains.stationary_distribution\nDiscreteMarkovChains.fundamental_matrix\nDiscreteMarkovChains.exit_probabilities\nDiscreteMarkovChains.first_passage_probabilities","category":"page"},{"location":"functions/probability/#DiscreteMarkovChains.stationary_distribution","page":"Probability Functions","title":"DiscreteMarkovChains.stationary_distribution","text":"stationary_distribution(x)\n\nDefinitions\n\nA stationary distribution of a Markov chain is a probability distribution that remains unchanged in the Markov chain as time progresses. It is a row vector, w such that its elements sum to 1 and it satisfies wT = w. T is the one-step transiton matrix of the Markov chain.\n\nIn other words, w is invariant by the matrix T.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA column vector, w, that satisfies the equation wT = w.\n\nReferences\n\nBrilliant.org\n\n\n\n\n\n","category":"function"},{"location":"functions/probability/#DiscreteMarkovChains.fundamental_matrix","page":"Probability Functions","title":"DiscreteMarkovChains.fundamental_matrix","text":"fundamental_matrix(x)\n\nDefinitions\n\nThe fundamental matrix of a markov chain is defined to be (I-C)^-1 where C is the sub-transition matrix that takes transient states to transient states.\n\nThe (i j)th entry of the fundamental matrix is the expected number of times the chain is in state j over the whole process given that the chain started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe fundamental matrix of the Markov chain\n\n\n\n\n\n","category":"function"},{"location":"functions/probability/#DiscreteMarkovChains.exit_probabilities","page":"Probability Functions","title":"DiscreteMarkovChains.exit_probabilities","text":"exit_probabilities(x)\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nAn array where element (i j) is the probability that transient state i will enter recurrent state j on its first step out of the transient states. That is, e_ij.\n\n\n\n\n\n","category":"function"},{"location":"functions/probability/#DiscreteMarkovChains.first_passage_probabilities","page":"Probability Functions","title":"DiscreteMarkovChains.first_passage_probabilities","text":"first_passage_probabilities(x, t, i=missing, j=missing)\n\nDefinitions\n\nThis is the probability that the process enters state j for the first time at time t given that the process started in state i at time 0. That is, f^(t)_ij. If no i or j is given, then it will return a matrix instead with entries f^(t)_ij for i and j in the state space of x.\n\nWhy Do We Use A Slow Algorithm?\n\nSo that t can be symbolic if nessesary. That is, if symbolic math libraries want to use this library, it will pose no hassle.\n\nArguments\n\nx: some kind of Markov chain.\nt: the time to calculate the first passage probability.\ni: the state that the prcess starts in.\nj: the state that the process must reach for the first time.\n\nReturns\n\nA scalar value or a matrix depending on whether i and j are given.\n\nReferences\n\nUniversity of Windsor\nDurham University\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#Core-Types","page":"Core Types","title":"Core Types","text":"","category":"section"},{"location":"functions/basic/#Contents","page":"Core Types","title":"Contents","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"Pages = [\"basic.md\"]","category":"page"},{"location":"functions/basic/#Index","page":"Core Types","title":"Index","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"Pages = [\"basic.md\"]","category":"page"},{"location":"functions/basic/#Documentation","page":"Core Types","title":"Documentation","text":"","category":"section"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"The main type is DiscreteMarkovChain:","category":"page"},{"location":"functions/basic/","page":"Core Types","title":"Core Types","text":"DiscreteMarkovChains.DiscreteMarkovChain\nDiscreteMarkovChains.state_space\nDiscreteMarkovChains.transition_matrix","category":"page"},{"location":"functions/basic/#DiscreteMarkovChains.DiscreteMarkovChain","page":"Core Types","title":"DiscreteMarkovChains.DiscreteMarkovChain","text":"DiscreteMarkovChain(transition_matrix)\nDiscreteMarkovChain(state_space, transition_matrix)\n\nCreates a new discrete Markov chain object.\n\nArguments\n\nstate_space: The names of the states that make up the Markov chain.\ntransition_matrix: The single step transition probability matrix.\n\nExamples\n\nThe following shows a basic Sunny-Cloudy-Rainy weather model.\n\nusing DiscreteMarkovChains\nT = [\n    0.9 0.1 0;\n    0.5 0.2 0.3;\n    0.1 0.4 0.5\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\nprintln(state_space(X))\n\n# output\n\n[\"Sunny\", \"Cloudy\", \"Rainy\"]\n\nprintln(transition_matrix(X))\n\n# output\n\n[0.9 0.1 0.0; 0.5 0.2 0.3; 0.1 0.4 0.5]\n\nReferences\n\nWikipedia\nDartmouth College\n\n\n\n\n\n","category":"type"},{"location":"functions/basic/#DiscreteMarkovChains.state_space","page":"Core Types","title":"DiscreteMarkovChains.state_space","text":"state_space(x)\n\nDefinitions\n\nThe state space of a Markov chain is the (ordered) set of values that the process is able to take on. For example, in a Sunny-Cloudy-Rainy weather model, the state space is [\"Sunny\", \"Cloudy\", \"Rainy\"].\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe state space of the Markov chain.\n\n\n\n\n\n","category":"function"},{"location":"functions/basic/#DiscreteMarkovChains.transition_matrix","page":"Core Types","title":"DiscreteMarkovChains.transition_matrix","text":"transition_matrix(x)\n\nDefinitions\n\nThe one-step transition matrix, T, of a Markov chain, X_t is a matrix whose (ij)th entry is the probability of the process being in state j at time 1 given that the process started in state i at time 0. That is T = p^(1)_ij = mathbbP(X_1=j  X_0=i).\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nThe transition matrix of the Markov chain.\n\n\n\n\n\n","category":"function"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributing is welcomed with open arms. This helps the package become better and simply by working on it, you are increasing its popularity.","category":"page"},{"location":"contributing/#Coding-Style","page":"Contributing","title":"Coding Style","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"One should see the Julia style guide as an introduction. The overall style used it the Blue style guide.","category":"page"},{"location":"contributing/#Exceptions-To-Blue","page":"Contributing","title":"Exceptions To Blue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Instead of leading underscores for names of private functions, we keep them as is. For example is_row_stochastic should be called _is_row_stochastic under the Blue style guide. This function is simply not exported. Since Julia allows for massive code reuse, there is a chance that a developer might want to make use of our private functions (which are actually more useful than the public ones for this package).\nWe will implicitly return nothing in functions. The Blue style guide requires this to be explicit. This is to make code look cleaner.\nInline comments that do not form a full sentence can start with a small letter or a capital letter. They must not end with a full stop. This is to match with the Blue style guide and with Julia's source code itself.","category":"page"},{"location":"contributing/#Additions-To-Blue","page":"Contributing","title":"Additions To Blue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All struct types should inherit from an abstract type. This makes inheritance easier for other developers using our code.\nOnly have abstract types in type annotations in functions and struct types. This makes it easier for other developers using our code.\nTry to make every element have the same character length in a matrix. See the following\n# yes\nT = [\n    0.0 1.0;\n    0.5 2.3;\n]\n # yes\nT = [\n    .12 .35;\n    .55 .90;\n]\n # no\nT = [\n    0 1.0;\n    0.5 2.3;\n]\n # no\nT = [\n    .12 .35;\n    .55 .9;\n]\nThis makes reading matrices easier. If you are ever unsure, do whatever looks the best.\nThe following headings are to be used for docstrings:\nThe methods of the function indented by 4 spaces.\nA brief explanation on the function.\nDefinitions\nParameters\nKeywords\nReturns\nThrows\nNotes\nExamples\nReferences\nNot all of these need to be present, but where applicable, they should follow this order. Headings must have a single empty line directly above them. They should come directly after a single hash (#) and space on the same line. They must have no empty lines directly below them.","category":"page"},{"location":"functions/simple/#Boolean-Functions","page":"Boolean Functions","title":"Boolean Functions","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"These functions have a boolean output. They are fairly simple and tend to categorize Markov chains into various labels.","category":"page"},{"location":"functions/simple/#Contents","page":"Boolean Functions","title":"Contents","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"Pages = [\"simple.md\"]","category":"page"},{"location":"functions/simple/#Index","page":"Boolean Functions","title":"Index","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"Pages = [\"simple.md\"]","category":"page"},{"location":"functions/simple/#Documentation","page":"Boolean Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/simple/","page":"Boolean Functions","title":"Boolean Functions","text":"DiscreteMarkovChains.is_regular\nDiscreteMarkovChains.is_ergodic\nDiscreteMarkovChains.is_absorbing","category":"page"},{"location":"functions/simple/#DiscreteMarkovChains.is_regular","page":"Boolean Functions","title":"DiscreteMarkovChains.is_regular","text":"is_regular(x)\n\nDefinitions\n\nA Markov chain is called a regular chain if some power of the transition matrix has only positive elements. This is equivalent to being ergodic and aperiodic.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is regular.\n\n\n\n\n\n","category":"function"},{"location":"functions/simple/#DiscreteMarkovChains.is_ergodic","page":"Boolean Functions","title":"DiscreteMarkovChains.is_ergodic","text":"is_ergodic(x)\n\nDefinitions\n\nA Markov chain is called an ergodic chain if it is possible to go from every state to every state (not necessarily in one move). That is, every state communicates and the whole tranistion matrix forms one communication class.\n\nIn many books, ergodic Markov chains are called irreducible.\n\nIf a Markov chain is not irreducible then it is reducible. This means that the Markov chain can be broken down into smaller irreducible chains that do not communicate. One chain might still be accessible from another though.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is ergodic. This is, if every state can be accessed from every other state. Another term for this is irreducible.\n\n\n\n\n\n","category":"function"},{"location":"functions/simple/#DiscreteMarkovChains.is_absorbing","page":"Boolean Functions","title":"DiscreteMarkovChains.is_absorbing","text":"is_absorbing(x)\n\nDefinitions\n\nA Markov chain is absorbing if it has at least one absorbing state, and if from every state it is possible to go to an absorbing state (not necessarily in one step).\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\ntrue if the Markov chain, x, is an absorbing chain. So the process is guarenteed to be absorbed eventually.\n\nReferences\n\nDartmouth College\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#Mean-Time-Functions","page":"Mean Time Functions","title":"Mean Time Functions","text":"","category":"section"},{"location":"functions/mean/#Contents","page":"Mean Time Functions","title":"Contents","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"Pages = [\"mean.md\"]","category":"page"},{"location":"functions/mean/#Index","page":"Mean Time Functions","title":"Index","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"Pages = [\"mean.md\"]","category":"page"},{"location":"functions/mean/#Documentation","page":"Mean Time Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/mean/","page":"Mean Time Functions","title":"Mean Time Functions","text":"DiscreteMarkovChains.expected_time_to_absorption\nDiscreteMarkovChains.mean_recurrence_time\nDiscreteMarkovChains.mean_first_passage_time","category":"page"},{"location":"functions/mean/#DiscreteMarkovChains.expected_time_to_absorption","page":"Mean Time Functions","title":"DiscreteMarkovChains.expected_time_to_absorption","text":"expected_time_to_absorption(x)\n\nDefinitions\n\nThe expected time to absorption is the expected number of steps that the process will take while in the transient super class given that the process started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array where element i is the total number of revisits to transient state i before leaving the transient super class.\n\nNote\n\nIt is advised to have x in canonical form already. This is to avoid confusion of what states make up each element of the array ouput.\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#DiscreteMarkovChains.mean_recurrence_time","page":"Mean Time Functions","title":"DiscreteMarkovChains.mean_recurrence_time","text":"mean_recurrence_time(x)\n\nDefinitions\n\nThis is the expected number of steps for the process to return to state i given that the process started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array where the ith entry is the mean recurrence time of state i.\n\nNote\n\nx must be irreducible (i.e. ergodic).\n\n\n\n\n\n","category":"function"},{"location":"functions/mean/#DiscreteMarkovChains.mean_first_passage_time","page":"Mean Time Functions","title":"DiscreteMarkovChains.mean_first_passage_time","text":"mean_first_passage_time(x)\n\nDefinitions\n\nThis is the expected number of steps for the process to reach state j given that the process started in state i.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA matrix where the (ij)th entry is the mean recurrence time of state i. Diagonal elements are 0. The diagonals would have represented the mean recurrence time.\n\n\n\n\n\n","category":"function"},{"location":"#DiscreteMarkovChains","page":"Home","title":"DiscreteMarkovChains","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = DiscreteMarkovChains","category":"page"},{"location":"","page":"Home","title":"Home","text":"DiscreteMarkovChains is a package that supports various functions relating to discrete Markov chains. In particular, it deals with discrete-time discrete-space time-homogenous finite Markov chains.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A 100% pure-Julia stack library for functions and queries related to discrete Markov chains.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiscreteMarkovChains should be up on Julia's package registry.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simply type ] add DiscreteMarkovChains into the Julia REPL.","category":"page"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the documentation hosted on GitHub Pages.","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We want to find out if this chain is an absorbing chain.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DiscreteMarkovChains\n\ntransition_matrix = [\n    0.0 1.0 0.0;\n    0.5 0.0 0.5;\n    0.0 1.0 0.0;\n]\nchain = DiscreteMarkovChain(transition_matrix)\nis_absorbing(chain)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's try find the communication classes, see if they are recurrent and what their periodicity is.","category":"page"},{"location":"","page":"Home","title":"Home","text":"periodicities(chain)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This means that we have one communication class with 3 recurrent states. Their periodicity is 2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since we have a single communication class, we can calculate the mean recurrence times.","category":"page"},{"location":"","page":"Home","title":"Home","text":"mean_recurrence_time(chain)","category":"page"},{"location":"","page":"Home","title":"Home","text":"So the first and third states take an average of 4 time steps to return to itself. The second state takes an average of 2 steps to return to itself.","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Chris du Plessis","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MIT","category":"page"},{"location":"functions/internal/#Internal-Functions","page":"Internal Functions","title":"Internal Functions","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"These functions are not intended to be used by you, the user.","category":"page"},{"location":"functions/internal/#Contents","page":"Internal Functions","title":"Contents","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"functions/internal/#Index","page":"Internal Functions","title":"Index","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"functions/internal/#Documentation","page":"Internal Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"The main type is DiscreteMarkovChain:","category":"page"},{"location":"functions/internal/","page":"Internal Functions","title":"Internal Functions","text":"DiscreteMarkovChains.strongly_connected_components\nDiscreteMarkovChains.breadth_first_search\n\nDiscreteMarkovChains.digraph\n\nDiscreteMarkovChains.state_index\nDiscreteMarkovChains.is_row_stochastic","category":"page"},{"location":"functions/internal/#DiscreteMarkovChains.strongly_connected_components","page":"Internal Functions","title":"DiscreteMarkovChains.strongly_connected_components","text":"strongly_connected_components(V::Array, E::Array{Tuple})\n\nStrongly connected components of a directed graph in reverse topological order. Translated from sympy/utilities/iteratbles.py/stronglyconnectedcomponents.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.breadth_first_search","page":"Internal Functions","title":"DiscreteMarkovChains.breadth_first_search","text":"Computes the period of an irreducible transition matrix T must be a 2D array\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.digraph","page":"Internal Functions","title":"DiscreteMarkovChains.digraph","text":"digraph(x)\n\nCreates a digraph (directed graph) representation of a Markov chain.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA 1D array of 2-tuples. An element (i j) is in the array iff the transition matrix at (ij) is nonzero.\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.state_index","page":"Internal Functions","title":"DiscreteMarkovChains.state_index","text":"state_index(x)\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA dictionary mapping each state in a Markov chain to its position in the state space. It is essentially the inverse of state_space(x).\n\n\n\n\n\n","category":"function"},{"location":"functions/internal/#DiscreteMarkovChains.is_row_stochastic","page":"Internal Functions","title":"DiscreteMarkovChains.is_row_stochastic","text":"is_row_stochastic(mat, row_sum=1)\n\nTests whether a matrix, x, is row stochasitc. The desired sum of each row can be specified as well.\n\nDefinitions\n\nA matrix is said to be row stochasic if all its rows sum to 1. This Definitions is extened so that all its rows sum to row_sum.\n\nArguments\n\nmat: a matrix that we want to check.\nrow_sum: the desired value that each row should total to.\n\nReturns\n\ntrue if the given matrix, mat, is row-stochasitc.\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#Communication-Functions","page":"Communication Functions","title":"Communication Functions","text":"","category":"section"},{"location":"functions/communication/#Contents","page":"Communication Functions","title":"Contents","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"Pages = [\"communication.md\"]","category":"page"},{"location":"functions/communication/#Index","page":"Communication Functions","title":"Index","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"Pages = [\"communication.md\"]","category":"page"},{"location":"functions/communication/#Documentation","page":"Communication Functions","title":"Documentation","text":"","category":"section"},{"location":"functions/communication/","page":"Communication Functions","title":"Communication Functions","text":"DiscreteMarkovChains.communication_classes\nDiscreteMarkovChains.periodicities\nDiscreteMarkovChains.decompose\nDiscreteMarkovChains.canonical_form","category":"page"},{"location":"functions/communication/#DiscreteMarkovChains.communication_classes","page":"Communication Functions","title":"DiscreteMarkovChains.communication_classes","text":"communication_classes(x)\n\nDefinitions\n\nA state j is accessible from state i if it is possible to eventually reach state j given that the process started in state i. That is   t  mathbbN such that p^(t)_ij  0.\n\nStates i and j communicate if i is accessible from j and j is accessible from i.\n\nA communication class is the set of states in a Markov chain that are all accessible from one another. Communication classes form a class in the mathematical sense. They also form a partition of the state space.\n\nA state i is recurrent if the process is guarenteed to eventually return to state i given that the process started in state i. If a state i is not recurrent, then it is said to be transient.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple containing 2 arrays.\n\nThis first array contains C arrays which store the states that communicate.\nThe second array is an array of Bool where the ith value is true if the ith communication class is recurrent.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    1 0;\n    0 1;\n]\nX = DiscreteMarkovChain(T)\n\ncommunication_classes(X)\n\n# output\n\n([[1], [2]], Any[true, true])\n\nSo the Markov chain has two communication classes and both are recurrent.\n\nT = [\n    .5 .5 .0;\n    .2 .8 .0;\n    .1 .2 .7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\ncommunication_classes(X)\n\n# output\n\n([[\"Sunny\", \"Cloudy\"], [\"Rainy\"]], Any[true, false])\n\nSo the Sunny and Cloudy states communicate and are recurrent. The Rainy state is transient. Note that this is not a very good weather model since once it stops raining, it will never rain again.\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.periodicities","page":"Communication Functions","title":"DiscreteMarkovChains.periodicities","text":"periodicities(x::AbstractDiscreteMarkovChain)\n\nA more advanced version of communication_classes designed for discrete Markov chains. It is the same as communication_classes but it returns periodicities as well.\n\nDefinitions\n\nThe period, d_i of a state i is the greatest common denominator of all integers n  mathbbN for which p^(n)_ii  0. Written more succinctly,\n\nd_i = textgcd n  mathbbN  p^(n)_ii  0 \n\nIf d_i=1 then state i is said to be aperiodic.\n\nArguments\n\nx: some kind of discrete Markov chain.\n\nReturns\n\nA tuple containing 3 arrays.\n\nThis first array contains C arrays which store the states that communicate.\nThe second array is an array of Bool where the ith value is true if the ith communication class is recurrent.\nThe third array is the periodicity of each communication class.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    1 0;\n    0 1;\n]\nX = DiscreteMarkovChain(T)\n\nperiodicities(X)\n\n# output\n\n([[1], [2]], Any[true, true], Any[1, 1])\n\nSo the Markov chain has two communication classes and both are recurrent and aperiodic.\n\nT = [\n    0.0 1.0 0.0;\n    1.0 0.0 0.0;\n    0.1 0.2 0.7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\nperiodicities(X)\n\n# output\n\n([[\"Sunny\", \"Cloudy\"], [\"Rainy\"]], Any[true, false], Any[2, 1])\n\nSo the Sunny and Cloudy states communicate and are recurrent with period 2. The Rainy state is transient and aperiodic. Note that this is not a very good weather model since once it stops raining, it will never rain again. Also, each day after that, the process will oscillate between Sunny and Cloudy.\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.decompose","page":"Communication Functions","title":"DiscreteMarkovChains.decompose","text":"decompose(x)\n\nAll transition matrices can be reordered so that we have\n\nT = beginpmatrix\nA  0\nB  C\nendpmatrix\n\nWhere A, B and C are as described below. 0 is a matrix of zeros.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple of four values\n\nstates: the first value is an array of the new states.\nA: the second value is a matrix of recurrent states to recurrent states.\nB: the third value is a matrix of transient states to recurrent states.\nC: the fourth value is a matrix of transient to transient states.\n\nExamples\n\nusing DiscreteMarkovChains\nT = [\n    0.0 1.0 0.0;\n    1.0 0.0 0.0;\n    0.1 0.2 0.7;\n]\nX = DiscreteMarkovChain([\"Sunny\", \"Cloudy\", \"Rainy\"], T)\n\ndecompose(X)\n\n# output\n\n(Any[\"Sunny\", \"Cloudy\", \"Rainy\"], [0.0 1.0; 1.0 0.0], [0.1 0.2], [0.7])\n\n\n\n\n\n","category":"function"},{"location":"functions/communication/#DiscreteMarkovChains.canonical_form","page":"Communication Functions","title":"DiscreteMarkovChains.canonical_form","text":"canonical_form(x)\n\nReorders the states of the transition matrix of x so that recurrent states appear first and transient states appear last.\n\nArguments\n\nx: some kind of Markov chain.\n\nReturns\n\nA tuple with the new states and the new transition matrix.\n\nExamples\n\nWe will use the same matrix as previous examples but change the order of it.\n\nusing DiscreteMarkovChains\nT = [\n    0.7 0.2 0.1;\n    0.0 0.0 1.0;\n    0.0 1.0 0.0;\n]\nX = DiscreteMarkovChain([\"Rainy\", \"Cloudy\", \"Sunny\"], T)\n\ncanonical_form(X)\n\n# output\n\n(Any[\"Cloudy\", \"Sunny\", \"Rainy\"], [0.0 1.0 0.0; 1.0 0.0 0.0; 0.2 0.1 0.7])\n\nHere, Cloudy and Sunny are recurrent states so they appear first. Rainy is a transient state so it appears last.\n\n\n\n\n\n","category":"function"}]
}
